version: 1
default_environment: prod
project_id: b77cad0a-ffd8-494e-9e6b-18d6eacb71d4
send_anonymous_usage_stats: false
plugins:
  extractors:
  - name: tap-getpocket
    variant: edgarrmondragon
    pip_url: git+https://github.com/edgarrmondragon/tap-getpocket.git
  - name: tap-stackexchange
    variant: meltanolabs
    pip_url: git+https://github.com/MeltanoLabs/tap-stackexchange.git
    config:
      tags:
      - meltano
    select:
    - questions.*
  - name: tap-readthedocs
    variant: edgarrmondragon
    pip_url: git+https://github.com/edgarrmondragon/tap-readthedocs.git
    select:
    - projects.*
  loaders:
  - name: target-bigquery
    namespace: bigquery
    pip_url: pipelinewise-target-bigquery
    settings:
    - name: project_id
      description: BigQuery project
    - name: location
      value: US
      description: Region where BigQuery stores your dataset
    - name: default_target_schema
      value: $MELTANO_EXTRACT__LOAD_SCHEMA
      description: >
        Name of the schema where the tables will be created. If `schema_mapping`
        is not defined then every stream sent by the tap is loaded into this schema.
    - name: default_target_schema_select_permission
      description: >
        Grant USAGE privilege on newly created schemas and grant SELECT
        privilege on newly created
    - name: schema_mapping
      kind: object
      description: >
        Useful if you want to load multiple streams from one tap to multiple
        BigQuery schemas.
    - name: batch_size_rows
      kind: integer
      value: 100000
      description: >
        Maximum number of rows in each batch. At the end of each batch,
        the rows in the batch are loaded into BigQuery.\n"
    - name: batch_wait_limit_seconds
      kind: integer
      description: Maximum time to wait for batch to reach batch_size_rows.
    - name: flush_all_streams
      kind: boolean
      value: false
    - name: parallelism
      kind: integer
      value: 0
    - name: max_parallelism
      kind: integer
      value: 16
    - name: add_metadata_columns
      kind: boolean
      value: true
      description: Metadata columns add extra row level information about data ingestions
    - name: credentials
      env: GOOGLE_APPLICATION_CREDENTIALS
      description: Path to Google API credentials file
    config:
      project_id: $BQ_PROJECT_ID
      location: $BQ_LOCATION
      credentials: ${MELTANO_PROJECT_ROOT}/.secrets/credentials.json
    dialect: bigquery
  - name: target-jsonl
    variant: andyh1203
    pip_url: target-jsonl
  - name: target-duckdb
    variant: jwills
    pip_url: target-duckdb~=0.4
    config:
      filepath: output/wh.duckdb
      default_target_schema: $MELTANO_ENVIRONMENT
  transformers:
  - name: dbt-bigquery
    namespace: dbt_bigquery
    pip_url: dbt-core~=1.4.1 dbt-bigquery~=1.4.0
    executable: dbt
    settings:
    - name: project_dir
      value: $MELTANO_PROJECT_ROOT/transform
    - name: profiles_dir
      env_aliases:
      - DBT_PROFILES_DIR
      value: $MELTANO_PROJECT_ROOT/transform/profiles/bigquery
    - name: project_id
    - name: dataset_id
    - name: keyfile
    - name: location
    commands:
      run:
        args: run
        description: Compile SQL and execute against the current target database.
      compile: compile
      docs-serve: docs serve
      docs-generate: docs generate
      clean:
        args: clean
        description: Delete all folders in the clean-targets list (usually the dbt_modules
          and target directories.)
      test:
        args: test
        description: Test dbt models
    config:
      target: bigquery
      project_id: $BQ_PROJECT_ID
      dataset_id: ${MELTANO_ENVIRONMENT}
      keyfile: ${MELTANO_PROJECT_ROOT}/.secrets/credentials.json
      location: $BQ_LOCATION
  utilities:
  - name: sqlfluff
    variant: sqlfluff
    pip_url: dbt-core~=1.4.1 dbt-bigquery~=1.4.0 sqlfluff>=2.0.0a4 sqlfluff-templater-dbt>=2.0.0a4
    settings:
    - name: keyfile
      env_aliases:
      - DBT_BIGQUERY_KEYFILE
    - name: project_id
      env_aliases:
      - DBT_BIGQUERY_PROJECT_ID
    - name: dataset_id
      env_aliases:
      - DBT_BIGQUERY_DATASET_ID
    - name: location
      env_aliases:
      - DBT_BIGQUERY_LOCATION
    config:
      project_id: $BQ_PROJECT_ID
      dataset_id: ${MELTANO_ENVIRONMENT}
      keyfile: ${MELTANO_PROJECT_ROOT}/.secrets/credentials.json
      location: $BQ_LOCATION
  - name: evidence
    variant: meltanolabs
    pip_url: evidence-ext>=0.5
    config:
      home_dir: $MELTANO_PROJECT_ROOT/evidence
      settings:
        database: bigquery
environments:
- name: dev
  config:
    plugins:
      loaders:
      - name: target-bigquery
        config:
          batch_size_rows: 100
- name: staging
- name: prod
  config:
    plugins:
      loaders:
      - name: target-bigquery
        config:
          batch_size_rows: 10000
jobs:
- name: pocket-to-bq
  tasks:
  - tap-getpocket target-bigquery
- name: pocket-to-duckdb
  tasks:
  - tap-getpocket target-duckdb
- name: stackoverflow-to-bq
  tasks:
  - tap-stackexchange target-bigquery
- name: rtd-to-bq
  tasks:
  - tap-readthedocs target-bigquery
